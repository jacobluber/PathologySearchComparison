{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5bc4813-02eb-4a89-bcda-66362081f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import os\n",
    "from os import makedirs\n",
    "from os.path import join, splitext, dirname\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ssl_encoder_training import PairCenterDataset\n",
    "from utils.model.base_model import AttenHashEncoder\n",
    "from utils.evaluate import Evaluator\n",
    "from utils.retrieval_utils import generate_incidence\n",
    "from utils.evaluate import hamming_retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df3b966-fec6-48f0-b460-dcbe75192137",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca3bed1-2eb9-48de-a181-832c29a24ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(raw, model_dir):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder = AttenHashEncoder(feature_in, feature_out, depth)\n",
    "    model_path = os.path.join(model_dir, 'model_best.pth')\n",
    "    encoder.load_state_dict(torch.load(model_path))\n",
    "    encoder = encoder.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        raw = torch.from_numpy(raw).to(device)\n",
    "        h, w = encoder(raw, no_pooling=True, weight=True)\n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335d5360-15be-40d9-81d1-9d74289b48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyedge_similarity(inc, alpha, beta):\n",
    "    sh = generate_sh(inc)\n",
    "    sv = generate_sv(inc)\n",
    "    ss = inc + inc.T\n",
    "    sh, sv, ss = normalize(sh), normalize(sv), normalize(ss)\n",
    "\n",
    "    simi = sh + alpha * sv + beta * ss\n",
    "    simi_tops, slide_top_idx = torch.topk(torch.from_numpy(simi), simi.shape[0], dim=1, largest=True)\n",
    "\n",
    "    return slide_top_idx, simi_tops\n",
    "\n",
    "def generate_sh(inc):\n",
    "    return inc.dot(np.transpose(inc))\n",
    "\n",
    "def generate_sv(inc):\n",
    "    return np.transpose(inc).dot(inc)\n",
    "\n",
    "def normalize(mat):\n",
    "    row_sums = mat.sum(axis=1)\n",
    "    new_matrix = mat / row_sums[:, np.newaxis]\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d6eb98d-5ebe-4f91-990a-7314b035da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperGraph():\n",
    "    def __init__(self, experiment):\n",
    "        self.experiment = experiment\n",
    "        self.result_dict = {}\n",
    "        self.weight = None\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "        \n",
    "\n",
    "    def add_patches(self, patches, paths):\n",
    "        assert len(patches.shape) == 3\n",
    "        assert patches.shape[0] == len(paths)\n",
    "        for i in range(patches.shape[0]):\n",
    "            self.add_patch(patches[i], paths[i])\n",
    "\n",
    "    \n",
    "    def add_patch(self, patch, path):\n",
    "        assert len(patch.shape) == 2\n",
    "        for j in range(patch.shape[0]):\n",
    "            self.result_dict[path + '@' + str(j)] = patch[j]\n",
    "\n",
    "    \n",
    "    def add_weight(self, weight):\n",
    "        weight = weight.reshape(-1)\n",
    "        assert len(self.result_dict) == weight.shape[0]\n",
    "        self.weight = weight\n",
    "\n",
    "    \n",
    "    def get_results(self, K=10, alpha=1, beta=1):\n",
    "        key_list, top_idx, dis_mat = hamming_retrieval(hypergraph.result_dict)\n",
    "        inc, list_slide_id = generate_incidence(key_list, top_idx, num_cluster, K, hypergraph.weight)\n",
    "        slide_top_idx, simi_tops =  hyedge_similarity(inc, alpha, beta)\n",
    "        return list_slide_id, slide_top_idx, simi_tops\n",
    "\n",
    "\n",
    "    def save_metrics(self, list_slide_id, slide_top_idx, simi_tops, extension=\"svs\", site_retrieval=False):\n",
    "        save_dir = join(\"TEST_DATA_RESULTS\", self.experiment)\n",
    "        makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        k = 10 if site_retrieval else 5\n",
    "\n",
    "        records = []\n",
    "        for id, slide_path in enumerate(list_slide_id):\n",
    "            if slide_path.startswith(\"TEST_DATA\"):\n",
    "                temp = []\n",
    "                top_slide_ids = slide_top_idx[id]\n",
    "                top_slide_sims = simi_tops[id]\n",
    "\n",
    "                query_name = f\"{slide_path.split('/')[-1]}.{extension}\"\n",
    "                query_site = QUERY_SLIDES[query_name]\n",
    "                query_diagnosis = QUERY_SUBTYPES[query_name]\n",
    "\n",
    "                temp.extend([query_name, query_site, query_diagnosis])\n",
    "\n",
    "                counter = 0\n",
    "                for r, sim in zip(top_slide_ids, top_slide_sims):\n",
    "                    rr = list_slide_id[r]\n",
    "                    if not rr.startswith(\"TEST_DATA\"):\n",
    "                        result_name = f\"{rr.split('/')[-1]}.svs\"\n",
    "                        result_site = rr.split('/')[-3]\n",
    "                        result_diagnosis = rr.split('/')[-2]\n",
    "                        result_similarity = sim.item()\n",
    "                        temp.extend([result_name, result_site, result_diagnosis, result_similarity])\n",
    "                        counter = counter + 1\n",
    "                        if counter == k:\n",
    "                            break\n",
    "\n",
    "                records.append(temp)\n",
    "\n",
    "        columns = [\"query_name\", \"query_site\", \"query_diagnosis\"]\n",
    "        [columns.extend([f\"ret_{i}_name\", f\"ret_{i}_site\", f\"ret_{i}_diagnosis\", f\"ret_{i}_dist\"]) for i in range(1, k + 1)]\n",
    "        df = pd.DataFrame.from_records(records, columns=columns)\n",
    "        if site_retrieval:\n",
    "            # save_path = join(save_dir, f\"site_{slide_path.split('/')[-1]}.csv\")\n",
    "            save_path = join(save_dir, f\"site.csv\")\n",
    "        else:\n",
    "            # save_path = join(save_dir, f\"sub_type_{slide_path.split('/')[-1]}.csv\")\n",
    "            save_path = join(save_dir, f\"sub_type.csv\")\n",
    "        df.to_csv(save_path, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bab38-7cc0-4231-8843-2c2024e4034e",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a08ef7-71bc-4e1c-8e8e-b6ef0227010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cluster = 20\n",
    "feature_in = 512\n",
    "feature_out = 1024\n",
    "depth = 1\n",
    "\n",
    "# MODEL_DIR = \"/home/mxn2498/projects/new_search_comp/hshr/FEATURES/DATABASE/model/ssl_att\"\n",
    "MODEL_DIR = \"checkpoints\"\n",
    "RESULT_DIR = \"/home/mxn2498/projects/new_search_comp/hshr/FEATURES/DATABASE\"\n",
    "TMP = \"/home/mxn2498/projects/new_search_comp/hshr/FEATURES/DATABASE/TEMP\"\n",
    "\n",
    "DATASETS = [\"brain\", \"breast\", \"colon\", \"liver\", \"lung\"]\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    \"brain\": [\"LGG\", \"GBM\"],\n",
    "    \"breast\": [\"BRCA\"],\n",
    "    \"lung\": [\"LUAD\", \"LUSC\"],\n",
    "    \"colon\": [\"COAD\"],\n",
    "    \"liver\": [\"LIHC\", \"CHOL\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad888d5-da64-489c-a66c-865b7ca1274a",
   "metadata": {},
   "source": [
    "# Site Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af9705af-1086-45f4-8db0-507eae09d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"UCLA\", \"READER_STUDY\", \"BRCA_HER2\", \"BRCA_TRASTUZUMAB\", \"GBM_MICROSCOPE_CPTAC\", \"GBM_MICROSCOPE_UPENN\"]\n",
    "extensions = [\"svs\", \"svs\", \"svs\", \"svs\", \"svs\", \"ndpi\"]\n",
    "\n",
    "TEST_RESULT_DIR = \"/home/mxn2498/projects/new_search_comp/hshr/FEATURES\"\n",
    "TEST_TMP = \"/home/mxn2498/projects/new_search_comp/hshr/FEATURES/TEST_DATA/TEMP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7857d20a-ce3f-4859-bbe1-285b1a48943e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for experiment, extension in zip(experiments, extensions):\n",
    "    with open(f\"FEATURES/TEST_DATA/{experiment}/query_slides.yaml\", 'r') as f:\n",
    "        QUERY_SLIDES = yaml.safe_load(f)\n",
    "    with open(f\"FEATURES/TEST_DATA/{experiment}/query_subtypes.yaml\", 'r') as f:\n",
    "        QUERY_SUBTYPES = yaml.safe_load(f)\n",
    "\n",
    "    cfs = []\n",
    "    cf_paths = []\n",
    "    for name in DATASETS:\n",
    "        valid_dataset = PairCenterDataset(RESULT_DIR, TMP, False, EXPERIMENTS[name])\n",
    "        for c1, _, path in valid_dataset.centers:\n",
    "            cfs.append(c1)\n",
    "            cf_paths.append(path)\n",
    "            \n",
    "    valid_dataset = PairCenterDataset(TEST_RESULT_DIR, TEST_TMP, False, [experiment])\n",
    "    for c1, _, path in valid_dataset.centers:\n",
    "        cfs.append(c1)\n",
    "        cf_paths.append(path)\n",
    "    cfs = np.array(cfs)\n",
    "    \n",
    "    hypergraph = HyperGraph(experiment)\n",
    "    \n",
    "    h, w = fine_tune(cfs, MODEL_DIR)\n",
    "    hypergraph.add_patches(h.cpu().detach().numpy(), cf_paths)\n",
    "    hypergraph.add_weight(w.cpu().detach().numpy())\n",
    "    \n",
    "    list_slide_id, slide_top_idx, simi_tops = hypergraph.get_results(K=10, alpha=1, beta=1)\n",
    "    hypergraph.save_metrics(list_slide_id, slide_top_idx, simi_tops, extension, site_retrieval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0402029-aa49-4b9f-b188-ac475b64a27f",
   "metadata": {},
   "source": [
    "# Subtype Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b0462d-43bc-421e-a7d5-dfaad8f975f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"BRCA_HER2\", \"BRCA_TRASTUZUMAB\", \"GBM_MICROSCOPE_CPTAC\", \"GBM_MICROSCOPE_UPENN\"]\n",
    "extensions = [\"svs\", \"svs\", \"svs\", \"ndpi\"]\n",
    "sites = [\"breast\", \"breast\", \"brain\", \"brain\"]\n",
    "\n",
    "TEST_RESULT_DIR = \"/home/mxn2498/projects/new_search_comp/hshr/FEATURES\"\n",
    "TEST_TMP = \"/home/mxn2498/projects/new_search_comp/hshr/FEATURES/TEST_DATA/TEMP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11cc0eeb-6e73-4db6-8d11-eeb874c8ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment, extension, site in zip(experiments, extensions, sites):\n",
    "    with open(f\"FEATURES/TEST_DATA/{experiment}/query_slides.yaml\", 'r') as f:\n",
    "        QUERY_SLIDES = yaml.safe_load(f)\n",
    "    with open(f\"FEATURES/TEST_DATA/{experiment}/query_subtypes.yaml\", 'r') as f:\n",
    "        QUERY_SUBTYPES = yaml.safe_load(f)  \n",
    "    \n",
    "    cfs = []\n",
    "    cf_paths = []\n",
    "    name = site\n",
    "    valid_dataset = PairCenterDataset(RESULT_DIR, TMP, False, EXPERIMENTS[name])\n",
    "    for c1, _, path in valid_dataset.centers:\n",
    "        cfs.append(c1)\n",
    "        cf_paths.append(path)\n",
    "    valid_dataset = PairCenterDataset(TEST_RESULT_DIR, TEST_TMP, False, [experiment])\n",
    "    for c1, _, path in valid_dataset.centers:\n",
    "        cfs.append(c1)\n",
    "        cf_paths.append(path)\n",
    "    cfs = np.array(cfs)\n",
    "    \n",
    "    hypergraph = HyperGraph(experiment)\n",
    "    \n",
    "    h, w = fine_tune(cfs, MODEL_DIR)\n",
    "    hypergraph.add_patches(h.cpu().detach().numpy(), cf_paths)\n",
    "    hypergraph.add_weight(w.cpu().detach().numpy())\n",
    "    \n",
    "    list_slide_id, slide_top_idx, simi_tops = hypergraph.get_results(K=10, alpha=1, beta=1)\n",
    "    hypergraph.save_metrics(list_slide_id, slide_top_idx, simi_tops, extension, site_retrieval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f025b1d-9034-4ae9-8fc4-c8e415b11db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"UCLA\", \"READER_STUDY\"]\n",
    "extensions = [\"svs\", \"svs\"]\n",
    "\n",
    "TEST_RESULT_DIR = \"/home/mxn2498/projects/new_search_comp/hshr/FEATURES/TEST_DATA\"\n",
    "TEST_TMP = \"/home/mxn2498/projects/new_search_comp/hshr/FEATURES/TEST_DATA/TEMP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "179aaa13-d627-4992-a8da-54f1224f7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment, extension, site in zip(experiments, extensions, sites):\n",
    "    with open(f\"FEATURES/TEST_DATA/{experiment}/query_slides.yaml\", 'r') as f:\n",
    "        QUERY_SLIDES = yaml.safe_load(f)\n",
    "    with open(f\"FEATURES/TEST_DATA/{experiment}/query_subtypes.yaml\", 'r') as f:\n",
    "        QUERY_SUBTYPES = yaml.safe_load(f)  \n",
    "\n",
    "    for slide, site in QUERY_SLIDES.items():\n",
    "        cfs = []\n",
    "        cf_paths = []\n",
    "        name = site\n",
    "        valid_dataset = PairCenterDataset(RESULT_DIR, TMP, False, EXPERIMENTS[name])\n",
    "        for c1, _, path in valid_dataset.centers:\n",
    "            cfs.append(c1)\n",
    "            cf_paths.append(path)\n",
    "\n",
    "        f0 = join(TEST_RESULT_DIR, experiment, splitext(slide)[0], \"clu_0.npy\")\n",
    "        f1 = join(TEST_RESULT_DIR, experiment, splitext(slide)[0], \"clu_1.npy\") \n",
    "        c0 = np.load(f0)\n",
    "        c1 = np.load(f1)\n",
    "        path = f\"TEST_DATA/{experiment}/{splitext(slide)[0]}\"\n",
    "        cfs.append(c0)\n",
    "        cf_paths.append(path)\n",
    "        cfs = np.array(cfs)\n",
    "\n",
    "        new_exp = f\"{experiment}_{splitext(slide)[0]}\"\n",
    "        hypergraph = HyperGraph(new_exp)\n",
    "        \n",
    "        h, w = fine_tune(cfs, MODEL_DIR)\n",
    "        hypergraph.add_patches(h.cpu().detach().numpy(), cf_paths)\n",
    "        hypergraph.add_weight(w.cpu().detach().numpy())\n",
    "        \n",
    "        list_slide_id, slide_top_idx, simi_tops = hypergraph.get_results(K=10, alpha=1, beta=1)\n",
    "        hypergraph.save_metrics(list_slide_id, slide_top_idx, simi_tops, extension, site_retrieval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f024bb-2802-49a7-8d0d-dedae65ec7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
