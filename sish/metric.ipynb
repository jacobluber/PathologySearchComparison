{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de060315-5732-49f4-b6c3-aef487a764ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from os import makedirs\n",
    "from os.path import join, basename, splitext\n",
    "import pickle\n",
    "import glob\n",
    "import operator\n",
    "import copy\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import datetime\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f28c8b-285b-4909-b033-d93c324efceb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b25e170-83d9-4e9e-9665-b6fe6d11e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_object(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def query(results, site, topK_mMV):\n",
    "    query_results = []\n",
    "    for test_slide in results.keys():\n",
    "        test_slide_result = results[test_slide]['results']\n",
    "        \n",
    "        # Filter out complete failure case (i.e.,\n",
    "        # All mosaics fail to retrieve a patch that meet the criteria)\n",
    "        ttlen = 0\n",
    "        for tt in test_slide_result:\n",
    "            ttlen += len(tt)\n",
    "        if ttlen == 0:\n",
    "            continue\n",
    "\n",
    "        bag_result = []\n",
    "        bag_summary = []\n",
    "        len_info = []\n",
    "        label_count_summary = {}\n",
    "        weight = calculate_weights(site)\n",
    "        for idx, bag in enumerate(test_slide_result):\n",
    "            if site == \"organ\":\n",
    "                ent, label_cnt, dist = Uncertainty_Cal(bag, weight, is_organ=True)\n",
    "            else:\n",
    "                ent, label_cnt, dist = Uncertainty_Cal(bag, weight, is_organ=False)\n",
    "\n",
    "            if ent is not None:\n",
    "                label_count_summary[idx] = label_cnt\n",
    "                bag_summary.append((idx, ent, dist, len(bag)))\n",
    "                len_info.append(len(bag))\n",
    "\n",
    "        bag_summary_dirty = copy.deepcopy(bag_summary)\n",
    "        bag_summary, hamming_thrsh = Clean(len_info, bag_summary)\n",
    "        bag_removed = Filtered_BY_Prediction(bag_summary, label_count_summary)\n",
    "\n",
    "        # Process to calculate the final ret slide\n",
    "        ret_final = []\n",
    "        visited = {}\n",
    "        for b in bag_summary:\n",
    "            bag_index = b[0]\n",
    "            uncertainty = b[1]\n",
    "            res = results[test_slide]['results'][bag_index]\n",
    "            for r in res:\n",
    "                if uncertainty == 0:\n",
    "                    if r['slide_name'] not in visited:\n",
    "                        if site == \"organ\":\n",
    "                            ret_final.append((r['slide_name'], r['hamming_dist'], r['site'], uncertainty, bag_index))\n",
    "                        else:\n",
    "                            ret_final.append((r['slide_name'], r['hamming_dist'], r['diagnosis'], uncertainty, bag_index))\n",
    "                        visited[r['slide_name']] = 1\n",
    "                else:\n",
    "                    if (r['hamming_dist'] <= hamming_thrsh) and (r['slide_name'] not in visited):\n",
    "                        if site == \"organ\":\n",
    "                            ret_final.append((r['slide_name'], r['hamming_dist'], r['site'], uncertainty, bag_index))\n",
    "                        else:\n",
    "                            ret_final.append((r['slide_name'], r['hamming_dist'], r['diagnosis'], uncertainty, bag_index))\n",
    "                        visited[r['slide_name']] = 1\n",
    "\n",
    "        ret_final_tmp = [(e[1], e[2], e[3], e[-1]) for e in sorted(ret_final, key=lambda x: (x[3], x[1]))\n",
    "                         if e[-1] not in bag_removed]\n",
    "        ret_final = [(e[0], e[1], e[2]) for e in sorted(ret_final, key=lambda x: (x[3], x[1]))\n",
    "                     if e[-1] not in bag_removed][0:topK_mMV]\n",
    "\n",
    "        query_results.append((test_slide, ret_final))\n",
    "    return query_results\n",
    "\n",
    "def calculate_weights(site):\n",
    "    if site == \"organ\":\n",
    "        factor = 30\n",
    "        # Count the number of slide in each diagnosis (organ)\n",
    "        latent_all = join(DATA_DIR, \"PATCHES\", \"*\", \"*\", \"*\", \"patches\", \"*\")\n",
    "        type_of_organ = [basename(e) for e in glob.glob(join(DATA_DIR, \"PATCHES\", \"*\"))]\n",
    "        total_slide = {k: 0 for k in type_of_organ}\n",
    "        for latent_path in glob.glob(latent_all):\n",
    "            anatomic_site = latent_path.split(\"/\")[-5]\n",
    "            total_slide[anatomic_site] += 1\n",
    "    else:\n",
    "        factor = 10\n",
    "        # Count the number of slide in each site (organ)\n",
    "        latent_all = join(DATA_DIR, \"PATCHES\", site, \"*\", \"*\", \"patches\", \"*\")\n",
    "        type_of_diagnosis = [basename(e) for e in glob.glob(join(DATA_DIR, \"PATCHES\", site, \"*\"))]\n",
    "        total_slide = {k: 0 for k in type_of_diagnosis}\n",
    "        for latent_path in glob.glob(latent_all):\n",
    "            diagnosis = latent_path.split(\"/\")[-4]\n",
    "            total_slide[diagnosis] += 1\n",
    "    \n",
    "    # Using the inverse count as a weight for each diagnosis\n",
    "    sum_inv = 0\n",
    "    for v in total_slide.values():\n",
    "        sum_inv += (1./v)\n",
    "\n",
    "    # Set a parameter k  to make the weight sum to k (k = 10, here)\n",
    "    norm_fact = factor / sum_inv\n",
    "    weight = {k: norm_fact * 1./v for k, v in total_slide.items()}\n",
    "    return weight\n",
    "\n",
    "def Uncertainty_Cal(bag, weight, is_organ=False):\n",
    "    \"\"\"\n",
    "    Implementation of Weighted-Uncertainty-Cal in the paper.\n",
    "    Input:\n",
    "        bag (list): A list of dictionary which contain the searhc results for each mosaic\n",
    "    Output:\n",
    "        ent (float): The entropy of the mosaic retrieval results\n",
    "        label_count (dict): The diagnois and the corresponding weight for each mosaic\n",
    "        hamming_dist (list): A list of hamming distance between the input mosaic and the result\n",
    "    \"\"\"\n",
    "    if len(bag) >= 1:\n",
    "        label = []\n",
    "        hamming_dist = []\n",
    "        label_count = defaultdict(float)\n",
    "        for bres in bag:\n",
    "            if is_organ:\n",
    "                label.append(bres['site'])\n",
    "            else:\n",
    "                label.append(bres['diagnosis'])\n",
    "            hamming_dist.append(bres['hamming_dist'])\n",
    "\n",
    "        # Counting the diagnoiss by weigted count\n",
    "        # If the count is less than 1, round to 1\n",
    "        for lb_idx, lb in enumerate(label):\n",
    "            label_count[lb] += (1. / (lb_idx + 1)) * weight[lb]\n",
    "        for k, v in label_count.items():\n",
    "            if v < 1.0:\n",
    "                v = 1.0\n",
    "            else:\n",
    "                label_count[k] = v\n",
    "\n",
    "        # Normalizing the count to [0,1] for entropy calculation\n",
    "        total = 0\n",
    "        ent = 0\n",
    "        for v in label_count.values():\n",
    "            total += v\n",
    "        for k in label_count.keys():\n",
    "            label_count[k] = label_count[k] / total\n",
    "        for v in label_count.values():\n",
    "            ent += (-v * np.log2(v))\n",
    "        return ent, label_count, hamming_dist\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "def Clean(len_info, bag_summary):\n",
    "    \"\"\"\n",
    "    Implementation of Clean in the paper\n",
    "    Input:\n",
    "        len_info (list): The length of retrieval results for each mosaic\n",
    "        bag_summary (list): A list that contains the positional index of mosaic,\n",
    "        entropy, the hamming distance list, and the length of retrieval results\n",
    "    Output:\n",
    "        bag_summary (list): The same format as input one but without low quality result\n",
    "        (i.e, result with large hamming distance)\n",
    "        top5_hamming_distance (float): The mean of average hamming distance in top 5\n",
    "        retrival results of all mosaics\n",
    "    \"\"\"\n",
    "    LOW_FREQ_THRSH = 3\n",
    "    LOW_PRECENT_THRSH = 5\n",
    "    HIGH_PERCENT_THRSH = 95\n",
    "    len_info = [b[-1] for b in bag_summary]\n",
    "    if len(set(len_info)) <= LOW_FREQ_THRSH:\n",
    "        pass\n",
    "    else:\n",
    "        bag_summary = [b for b in bag_summary if b[-1]\n",
    "                       > np.percentile(len_info, LOW_PRECENT_THRSH)\n",
    "                       and b[-1] < np.percentile(len_info, HIGH_PERCENT_THRSH)]\n",
    "\n",
    "    # Remove the mosaic if its top5 mean hammign distance is bigger than average\n",
    "    top5_hamming_dist = np.mean([np.mean(b[2][0:5]) for b in bag_summary])\n",
    "\n",
    "    bag_summary = sorted(bag_summary, key=lambda x: (x[1]))  # sort by certainty\n",
    "    bag_summary = [b for b in bag_summary if np.mean(b[2][0:5]) <= top5_hamming_dist]\n",
    "    return bag_summary, top5_hamming_dist\n",
    "\n",
    "def Filtered_BY_Prediction(bag_summary, label_count_summary):\n",
    "    \"\"\"\n",
    "    Implementation of Filtered_By_Prediction in the paper\n",
    "    Input:\n",
    "        bag_summary (list): The same as the output from Clean\n",
    "        label_count_summary (dict): The dictionary storing the diagnosis occurrence \n",
    "        of the retrieval result in each mosaic\n",
    "    Output:\n",
    "        bag_removed: The index (positional) of moaic that should not be considered \n",
    "        among the top5\n",
    "    \"\"\"\n",
    "    voting_board = defaultdict(float)\n",
    "    for b in bag_summary[0:5]:\n",
    "        bag_index = b[0]\n",
    "        for k, v in label_count_summary[bag_index].items():\n",
    "            voting_board[k] += v\n",
    "    final_vote_candidates = sorted(voting_board.items(), key=lambda x: -x[1])\n",
    "    fv_pointer = 0\n",
    "    while True:\n",
    "        final_vote = final_vote_candidates[fv_pointer][0]\n",
    "        bag_removed = {}\n",
    "        for b in bag_summary[0:5]:\n",
    "            bag_index = b[0]\n",
    "            max_vote = max(label_count_summary[bag_index].items(), key=operator.itemgetter(1))[0]\n",
    "            if max_vote != final_vote:\n",
    "                bag_removed[bag_index] = 1\n",
    "        if len(bag_removed) != len(bag_summary[0:5]):\n",
    "            break\n",
    "        else:\n",
    "            fv_pointer += 1\n",
    "    return bag_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3add31-3ccd-43fd-b455-719c08906227",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f222fb1f-59a6-4fd9-9034-2126e37df893",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_dict = {\n",
    "    \"Brain Lower Grade Glioma\": \"LGG\",\n",
    "    \"Glioblastoma Multiforme\": \"GBM\",\n",
    "    \"Breast Invasive Carcinoma\": \"BRCA\",\n",
    "    \"Lung Adenocarcinoma\": \"LUAD\",\n",
    "    \"Lung Squamous Cell Carcinoma\": \"LUSC\",\n",
    "    \"Colon Adenocarcinoma\": \"COAD\",\n",
    "    \"Liver Hepatocellular Carcinoma\": \"LIHC\",\n",
    "    \"Cholangiocarcinoma\": \"CHOL\",\n",
    "}\n",
    "\n",
    "sites_dict = {\n",
    "    \"Brain\": \"brain\",\n",
    "    \"Breast\": \"breast\",\n",
    "    \"Bronchus and lung\": \"lung\",\n",
    "    \"Colon\": \"colon\",\n",
    "    \"Liver and intrahepatic bile ducts\": \"liver\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab17a70d-bbb8-4209-b80e-ca840a973ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"FEATURES/DATABASE/\"\n",
    "\n",
    "DATASETS = [\"brain\", \"breast\", \"breast\", \"colon\", \"liver\", \"lung\"]\n",
    "EXPERIMENTS = [\"UCLA\", \"READER_STUDY\", \"BRCA_HER2\", \"BRCA_TRASTUZUMAB\", \"ABLATION_BRCA_TRASTUZUMAB\", \"GBM_MICROSCOPE_CPTAC\", \"GBM_MICROSCOPE_UPENN\"]\n",
    "EXTENSIONS = [\"svs\", \"svs\", \"svs\", \"svs\", \"svs\", \"svs\", \"ndpi\"]\n",
    "\n",
    "metadata = pd.read_csv(\"FEATURES/DATABASE/sampled_metadata.csv\")\n",
    "metadata = metadata.set_index('file_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d512042-e59d-43a6-ba27-9235750246e0",
   "metadata": {},
   "source": [
    "# Site Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "871aed1b-a8c4-4b3b-85ea-7abb4fe72da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"organ\"\n",
    "\n",
    "for experiment, extension in zip(EXPERIMENTS, EXTENSIONS):\n",
    "\n",
    "    k = 10\n",
    "    \n",
    "    with open(f\"FEATURES/TEST_DATA/{experiment}/query_slides.yaml\", 'r') as f:\n",
    "        QUERY_SLIDES = yaml.safe_load(f)\n",
    "    with open(f\"FEATURES/TEST_DATA/{experiment}/query_subtypes.yaml\", 'r') as f:\n",
    "        QUERY_SUBTYPES = yaml.safe_load(f)\n",
    "    \n",
    "    save_dir = join(\"TEST_DATA_RESULTS\", experiment)\n",
    "    makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    results_path = join(\"FEATURES/TEST_DATA/\", experiment, \"Results\", dataset, \"results.pkl\")\n",
    "    results = unpickle_object(results_path)\n",
    "    query_results = query(results, dataset, k)\n",
    "    \n",
    "    records = []\n",
    "    for returned in query_results:\n",
    "        query_name, result = returned\n",
    "        \n",
    "        temp = []\n",
    "        if experiment == \"UCLA\":\n",
    "            query_name = f\"{query_name}.{extension}\"\n",
    "            query_site = QUERY_SLIDES[query_name]\n",
    "            query_diagnosis = QUERY_SUBTYPES[query_name]\n",
    "        else:\n",
    "            query_name = f\"{query_name}.{extension}\"\n",
    "            query_site = QUERY_SLIDES[query_name]\n",
    "            query_diagnosis = QUERY_SUBTYPES[query_name]\n",
    "    \n",
    "        temp.extend([query_name, query_site, query_diagnosis])\n",
    "    \n",
    "        for r in result:\n",
    "            result_name = f\"{r[0]}.svs\"\n",
    "            result_site = sites_dict[metadata.loc[result_name, \"primary_site\"]]\n",
    "            result_diagnosis = diagnoses_dict[metadata.loc[result_name, \"project_name\"]]\n",
    "            result_distance = r[1]\n",
    "            \n",
    "            temp.extend([result_name, result_site, result_diagnosis, result_distance])\n",
    "    \n",
    "        records.append(temp)\n",
    "    \n",
    "    columns = [\"query_name\", \"query_site\", \"query_diagnosis\"]\n",
    "    [columns.extend([f\"ret_{i}_name\", f\"ret_{i}_site\", f\"ret_{i}_diagnosis\", f\"ret_{i}_dist\"]) for i in range(1, k + 1)]\n",
    "    \n",
    "    for record in records:\n",
    "        if len(record) < len(columns):\n",
    "            record.extend([None] * (len(columns) - len(record)))\n",
    "            \n",
    "    df = pd.DataFrame.from_records(records, columns=columns)\n",
    "    save_path = join(save_dir, f\"site.csv\")\n",
    "    df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e1564-0982-4eac-88ab-981d893250d3",
   "metadata": {},
   "source": [
    "# Sub Type Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7432c07-38a4-4d21-903a-d774ad6e4855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing brain_7316UP-19.ndpi started ...\n",
      "processing brain_7316UP-99.ndpi started ...\n",
      "processing brain_7316UP-213.ndpi started ...\n",
      "processing brain_7316UP-1302.ndpi started ...\n",
      "processing brain_7316UP-426.ndpi started ...\n",
      "processing brain_7316UP-435.ndpi started ...\n",
      "processing brain_7316UP-743.ndpi started ...\n",
      "processing brain_7316UP-1206.ndpi started ...\n",
      "processing brain_7316UP-613.ndpi started ...\n",
      "processing brain_7316UP-2726.ndpi started ...\n",
      "processing brain_7316UP-1405.ndpi started ...\n",
      "processing brain_7316UP-393.ndpi started ...\n",
      "processing brain_7316UP-3301.ndpi started ...\n",
      "processing brain_7316UP-2058.ndpi started ...\n",
      "processing brain_7316UP-498.ndpi started ...\n",
      "processing brain_7316UP-961.ndpi started ...\n",
      "processing brain_7316UP-895.ndpi started ...\n",
      "processing brain_7316UP-1883.ndpi started ...\n",
      "processing brain_7316UP-302.ndpi started ...\n",
      "processing brain_7316UP-1135.ndpi started ...\n",
      "processing brain_7316UP-1108.ndpi started ...\n",
      "processing brain_7316UP-243.ndpi started ...\n",
      "processing brain_7316UP-547.ndpi started ...\n",
      "processing brain_7316UP-2165.ndpi started ...\n",
      "processing brain_7316UP-1848.ndpi started ...\n",
      "processing brain_7316UP-3741.ndpi started ...\n",
      "processing brain_7316UP-2100.ndpi started ...\n",
      "processing brain_7316UP-109.ndpi started ...\n",
      "processing brain_7316UP-334.ndpi started ...\n",
      "processing brain_7316UP-504.ndpi started ...\n",
      "processing brain_7316UP-2923.ndpi started ...\n",
      "processing brain_7316UP-219.ndpi started ...\n",
      "processing brain_7316UP-349.ndpi started ...\n",
      "processing brain_7316UP-160.ndpi started ...\n"
     ]
    }
   ],
   "source": [
    "for experiment, extension in zip(EXPERIMENTS, EXTENSIONS):\n",
    "    k = 5\n",
    "    \n",
    "    with open(f\"FEATURES/TEST_DATA/{experiment}/query_slides.yaml\", 'r') as f:\n",
    "        QUERY_SLIDES = yaml.safe_load(f)\n",
    "    with open(f\"FEATURES/TEST_DATA/{experiment}/query_subtypes.yaml\", 'r') as f:\n",
    "        QUERY_SUBTYPES = yaml.safe_load(f)\n",
    "\n",
    "    save_dir = join(\"TEST_DATA_RESULTS\", experiment)\n",
    "    makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    records = []\n",
    "    for slide, site in QUERY_SLIDES.items():\n",
    "        print(f\"processing {site}_{slide} started ...\")\n",
    "        results_path = join(\"FEATURES/TEST_DATA/\", experiment, \"Results\", site, \"results.pkl\")\n",
    "        results = unpickle_object(results_path)\n",
    "        query_results = query(results, site, k)\n",
    "\n",
    "        temp = []\n",
    "        for query_name, result in query_results:\n",
    "            query_name = f\"{query_name}.{extension}\"\n",
    "            \n",
    "            if query_name == slide:\n",
    "                query_site = QUERY_SLIDES[query_name]\n",
    "                query_diagnosis = QUERY_SUBTYPES[query_name]\n",
    "                \n",
    "                temp.extend([query_name, query_site, query_diagnosis])\n",
    "\n",
    "                for r in result:\n",
    "                    result_name = f\"{r[0]}.svs\"\n",
    "                    result_site = sites_dict[metadata.loc[result_name, \"primary_site\"]]\n",
    "                    result_diagnosis = diagnoses_dict[metadata.loc[result_name, \"project_name\"]]\n",
    "                    result_distance = r[1]\n",
    "                    \n",
    "                    temp.extend([result_name, result_site, result_diagnosis, result_distance])\n",
    "\n",
    "        records.append(temp)\n",
    "\n",
    "    columns = [\"query_name\", \"query_site\", \"query_diagnosis\"]\n",
    "    [columns.extend([f\"ret_{i}_name\", f\"ret_{i}_site\", f\"ret_{i}_diagnosis\", f\"ret_{i}_dist\"]) for i in range(1, k + 1)]\n",
    "    \n",
    "    for record in records:\n",
    "        if len(record) < len(columns):\n",
    "            record.extend([None] * (len(columns) - len(record)))\n",
    "            \n",
    "    df = pd.DataFrame.from_records(records, columns=columns)\n",
    "    save_path = join(save_dir, f\"sub_type.csv\")\n",
    "    df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1d421-dfa5-41a3-8dec-770fdaf6a1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
