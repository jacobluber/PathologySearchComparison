{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e20ecf-15cc-483a-be2c-07d7e4012989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from os import makedirs\n",
    "from os.path import join, basename, splitext\n",
    "import pickle\n",
    "import glob\n",
    "import operator\n",
    "import copy\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import datetime\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from database import HistoDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1a7f9-5640-4108-9022-2fd50ac00e43",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b260d4d-6b15-48db-a638-8f8dbdcf1a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unpickle_object(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a7d73-b49a-4deb-8736-3a5740899d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_link(name):\n",
    "    fields = [\"file_name\"]\n",
    "    fields = \",\".join(fields)\n",
    "\n",
    "    # We want all svs files from 'Breas' primary site.\n",
    "    filters = {\n",
    "        \"op\": \"in\",\n",
    "        \"content\":{\n",
    "            \"field\": \"file_name\",\n",
    "            \"value\": [name + \".svs\"]\n",
    "            }\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"filters\": json.dumps(filters),\n",
    "        \"fields\": fields,\n",
    "        \"format\": \"CSV\",\n",
    "        \"size\": \"1\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(FILES_ENDPOINT, headers={\"Content-Type\": \"application/json\"}, json=params)\n",
    "    df = pd.read_csv(io.StringIO(response.content.decode('utf-8')), dtype='object')\n",
    "    \n",
    "    return VIEW_URL + df.id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6031707-860a-4d58-b707-954d1b47796c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Uncertainty_Cal(bag, weight, is_organ=False):\n",
    "    \"\"\"\n",
    "    Implementation of Weighted-Uncertainty-Cal in the paper.\n",
    "    Input:\n",
    "        bag (list): A list of dictionary which contain the searhc results for each mosaic\n",
    "    Output:\n",
    "        ent (float): The entropy of the mosaic retrieval results\n",
    "        label_count (dict): The diagnois and the corresponding weight for each mosaic\n",
    "        hamming_dist (list): A list of hamming distance between the input mosaic and the result\n",
    "    \"\"\"\n",
    "    if len(bag) >= 1:\n",
    "        label = []\n",
    "        hamming_dist = []\n",
    "        label_count = defaultdict(float)\n",
    "        for bres in bag:\n",
    "            if is_organ:\n",
    "                label.append(bres['site'])\n",
    "            else:\n",
    "                label.append(bres['diagnosis'])\n",
    "            hamming_dist.append(bres['hamming_dist'])\n",
    "\n",
    "        # Counting the diagnoiss by weigted count\n",
    "        # If the count is less than 1, round to 1\n",
    "        for lb_idx, lb in enumerate(label):\n",
    "            label_count[lb] += (1. / (lb_idx + 1)) * weight[lb]\n",
    "        for k, v in label_count.items():\n",
    "            if v < 1.0:\n",
    "                v = 1.0\n",
    "            else:\n",
    "                label_count[k] = v\n",
    "\n",
    "        # Normalizing the count to [0,1] for entropy calculation\n",
    "        total = 0\n",
    "        ent = 0\n",
    "        for v in label_count.values():\n",
    "            total += v\n",
    "        for k in label_count.keys():\n",
    "            label_count[k] = label_count[k] / total\n",
    "        for v in label_count.values():\n",
    "            ent += (-v * np.log2(v))\n",
    "        return ent, label_count, hamming_dist\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654e29f-4540-49dd-99ad-cb0dec7f36ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Clean(len_info, bag_summary):\n",
    "    \"\"\"\n",
    "    Implementation of Clean in the paper\n",
    "    Input:\n",
    "        len_info (list): The length of retrieval results for each mosaic\n",
    "        bag_summary (list): A list that contains the positional index of mosaic,\n",
    "        entropy, the hamming distance list, and the length of retrieval results\n",
    "    Output:\n",
    "        bag_summary (list): The same format as input one but without low quality result\n",
    "        (i.e, result with large hamming distance)\n",
    "        top5_hamming_distance (float): The mean of average hamming distance in top 5\n",
    "        retrival results of all mosaics\n",
    "    \"\"\"\n",
    "    LOW_FREQ_THRSH = 3\n",
    "    LOW_PRECENT_THRSH = 5\n",
    "    HIGH_PERCENT_THRSH = 95\n",
    "    len_info = [b[-1] for b in bag_summary]\n",
    "    if len(set(len_info)) <= LOW_FREQ_THRSH:\n",
    "        pass\n",
    "    else:\n",
    "        bag_summary = [b for b in bag_summary if b[-1]\n",
    "                       > np.percentile(len_info, LOW_PRECENT_THRSH)\n",
    "                       and b[-1] < np.percentile(len_info, HIGH_PERCENT_THRSH)]\n",
    "\n",
    "    # Remove the mosaic if its top5 mean hammign distance is bigger than average\n",
    "    top5_hamming_dist = np.mean([np.mean(b[2][0:5]) for b in bag_summary])\n",
    "\n",
    "    bag_summary = sorted(bag_summary, key=lambda x: (x[1]))  # sort by certainty\n",
    "    bag_summary = [b for b in bag_summary if np.mean(b[2][0:5]) <= top5_hamming_dist]\n",
    "    return bag_summary, top5_hamming_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c3692-de0b-4ded-bba2-521a8359f460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Filtered_BY_Prediction(bag_summary, label_count_summary):\n",
    "    \"\"\"\n",
    "    Implementation of Filtered_By_Prediction in the paper\n",
    "    Input:\n",
    "        bag_summary (list): The same as the output from Clean\n",
    "        label_count_summary (dict): The dictionary storing the diagnosis occurrence \n",
    "        of the retrieval result in each mosaic\n",
    "    Output:\n",
    "        bag_removed: The index (positional) of moaic that should not be considered \n",
    "        among the top5\n",
    "    \"\"\"\n",
    "    voting_board = defaultdict(float)\n",
    "    for b in bag_summary[0:5]:\n",
    "        bag_index = b[0]\n",
    "        for k, v in label_count_summary[bag_index].items():\n",
    "            voting_board[k] += v\n",
    "    final_vote_candidates = sorted(voting_board.items(), key=lambda x: -x[1])\n",
    "    fv_pointer = 0\n",
    "    while True:\n",
    "        final_vote = final_vote_candidates[fv_pointer][0]\n",
    "        bag_removed = {}\n",
    "        for b in bag_summary[0:5]:\n",
    "            bag_index = b[0]\n",
    "            max_vote = max(label_count_summary[bag_index].items(), key=operator.itemgetter(1))[0]\n",
    "            if max_vote != final_vote:\n",
    "                bag_removed[bag_index] = 1\n",
    "        if len(bag_removed) != len(bag_summary[0:5]):\n",
    "            break\n",
    "        else:\n",
    "            fv_pointer += 1\n",
    "    return bag_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f405b33-b632-4819-8f4b-334c7254ec0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_weights(site):\n",
    "    if site == \"organ\":\n",
    "        factor = 30\n",
    "        # Count the number of slide in each diagnosis (organ)\n",
    "        latent_all = join(DATA_DIR, \"PATCHES\", \"*\", \"*\", \"*\", \"patches\", \"*\")\n",
    "        type_of_organ = [basename(e) for e in glob.glob(join(DATA_DIR, \"PATCHES\", \"*\"))]\n",
    "        total_slide = {k: 0 for k in type_of_organ}\n",
    "        for latent_path in glob.glob(latent_all):\n",
    "            anatomic_site = latent_path.split(\"/\")[-5]\n",
    "            total_slide[anatomic_site] += 1\n",
    "    else:\n",
    "        factor = 10\n",
    "        # Count the number of slide in each site (organ)\n",
    "        latent_all = join(DATA_DIR, \"PATCHES\", site, \"*\", \"*\", \"patches\", \"*\")\n",
    "        type_of_diagnosis = [basename(e) for e in glob.glob(join(DATA_DIR, \"PATCHES\", site, \"*\"))]\n",
    "        total_slide = {k: 0 for k in type_of_diagnosis}\n",
    "        for latent_path in glob.glob(latent_all):\n",
    "            diagnosis = latent_path.split(\"/\")[-4]\n",
    "            total_slide[diagnosis] += 1\n",
    "    \n",
    "    # Using the inverse count as a weight for each diagnosis\n",
    "    sum_inv = 0\n",
    "    for v in total_slide.values():\n",
    "        sum_inv += (1./v)\n",
    "\n",
    "    # Set a parameter k  to make the weight sum to k (k = 10, here)\n",
    "    norm_fact = factor / sum_inv\n",
    "    weight = {k: norm_fact * 1./v for k, v in total_slide.items()}\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c78bb-8487-4f7a-840e-b42da6fdcbe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_results(query_slides, extension, site, experiment, pre_step, succ_step, C, T, thrsh, codebook_semantic=\"../../checkpoints/codebook_semantic.pt\"):  \n",
    "    save_dir = join(TEST_DATA_DIR, experiment, \"Results\", site)\n",
    "    makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    queries_latent_all = join(TEST_DATA_DIR, experiment, \"LATENT\", \"*\", \"*\", \"*\", \"vqvae\", \"*\")\n",
    "    \n",
    "    database_index_path = join(DATA_DIR, \"DATABASES\", site, \"index_tree\", \"veb.pkl\")\n",
    "    index_meta_path = join(DATA_DIR, \"DATABASES\", site, \"index_meta\", \"meta.pkl\")\n",
    "    db = HistoDatabase(database_index_path, index_meta_path, codebook_semantic)\n",
    "    \n",
    "    results = {}\n",
    "    for latent_path in glob.glob(queries_latent_all):\n",
    "        resolution = latent_path.split(\"/\")[-3]\n",
    "        diagnosis = latent_path.split(\"/\")[-4]\n",
    "        anatomic_site = latent_path.split(\"/\")[-5]\n",
    "        slide_id = basename(latent_path).replace(\".h5\", \"\")\n",
    "\n",
    "        densefeat_path = latent_path.replace(\"vqvae\", \"densenet\").replace(\".h5\", \".pkl\")\n",
    "        slide_path = os.path.join(WSI_DIR, anatomic_site, diagnosis, f\"{slide_id}.{extension}\")\n",
    "\n",
    "        db.leave_test_slides(list(query_slides.keys()))\n",
    "\n",
    "        with h5py.File(latent_path, 'r') as hf:\n",
    "            feat = hf['features'][:]\n",
    "            coords = hf['coords'][:]\n",
    "        with open(densefeat_path, 'rb') as handle:\n",
    "            densefeat = pickle.load(handle)\n",
    "\n",
    "        tmp_res = []\n",
    "        for idx, patch_latent in enumerate(feat):\n",
    "            res = db.query(patch_latent, densefeat[idx], pre_step, succ_step, C, T, thrsh)\n",
    "            tmp_res.append(res)\n",
    "\n",
    "        key = slide_id\n",
    "        results[key] = {'results': None, 'label_query': None}\n",
    "        results[key]['results'] = tmp_res\n",
    "        if site == 'organ':\n",
    "            results[key]['label_query'] = anatomic_site\n",
    "        else:\n",
    "            results[key]['label_query'] = diagnosis\n",
    "    \n",
    "    with open(join(save_dir, f\"results.pkl\"), 'wb') as handle:\n",
    "        pickle.dump(results, handle)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b284000-89ae-49ea-83a4-5f09ab17f943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query(results, site, topK_mMV):\n",
    "    query_results = []\n",
    "    for test_slide in results.keys():\n",
    "        test_slide_result = results[test_slide]['results']\n",
    "        \n",
    "        # Filter out complete failure case (i.e.,\n",
    "        # All mosaics fail to retrieve a patch that meet the criteria)\n",
    "        ttlen = 0\n",
    "        for tt in test_slide_result:\n",
    "            ttlen += len(tt)\n",
    "        if ttlen == 0:\n",
    "            continue\n",
    "\n",
    "        bag_result = []\n",
    "        bag_summary = []\n",
    "        len_info = []\n",
    "        label_count_summary = {}\n",
    "        weight = calculate_weights(site)\n",
    "        for idx, bag in enumerate(test_slide_result):\n",
    "            if site == \"organ\":\n",
    "                ent, label_cnt, dist = Uncertainty_Cal(bag, weight, is_organ=True)\n",
    "            else:\n",
    "                ent, label_cnt, dist = Uncertainty_Cal(bag, weight, is_organ=False)\n",
    "\n",
    "            if ent is not None:\n",
    "                label_count_summary[idx] = label_cnt\n",
    "                bag_summary.append((idx, ent, dist, len(bag)))\n",
    "                len_info.append(len(bag))\n",
    "\n",
    "        bag_summary_dirty = copy.deepcopy(bag_summary)\n",
    "        bag_summary, hamming_thrsh = Clean(len_info, bag_summary)\n",
    "        bag_removed = Filtered_BY_Prediction(bag_summary, label_count_summary)\n",
    "\n",
    "        # Process to calculate the final ret slide\n",
    "        ret_final = []\n",
    "        visited = {}\n",
    "        for b in bag_summary:\n",
    "            bag_index = b[0]\n",
    "            uncertainty = b[1]\n",
    "            res = results[test_slide]['results'][bag_index]\n",
    "            for r in res:\n",
    "                if uncertainty == 0:\n",
    "                    if r['slide_name'] not in visited:\n",
    "                        if site == \"organ\":\n",
    "                            ret_final.append((r['slide_name'], r['hamming_dist'], r['site'], uncertainty, bag_index))\n",
    "                        else:\n",
    "                            ret_final.append((r['slide_name'], r['hamming_dist'], r['diagnosis'], uncertainty, bag_index))\n",
    "                        visited[r['slide_name']] = 1\n",
    "                else:\n",
    "                    if (r['hamming_dist'] <= hamming_thrsh) and (r['slide_name'] not in visited):\n",
    "                        if site == \"organ\":\n",
    "                            ret_final.append((r['slide_name'], r['hamming_dist'], r['site'], uncertainty, bag_index))\n",
    "                        else:\n",
    "                            ret_final.append((r['slide_name'], r['hamming_dist'], r['diagnosis'], uncertainty, bag_index))\n",
    "                        visited[r['slide_name']] = 1\n",
    "\n",
    "        ret_final_tmp = [(e[1], e[2], e[3], e[-1]) for e in sorted(ret_final, key=lambda x: (x[3], x[1]))\n",
    "                         if e[-1] not in bag_removed]\n",
    "        ret_final = [(e[0], e[1], e[2]) for e in sorted(ret_final, key=lambda x: (x[3], x[1]))\n",
    "                     if e[-1] not in bag_removed][0:topK_mMV]\n",
    "\n",
    "        query_results.append((test_slide, ret_final))\n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd2858-c40d-4edb-8629-8ceaf1c327de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_results(test_slide, extension, ret_final, metadata, experiment, site_retrieval, topK_mMV):\n",
    "    if site_retrieval:\n",
    "        save_dir = join(\"../../TEST_DATA_RESULTS\", experiment, \"search_result_images\", \"wsi_site_retrieval\", query_slides_proxies[test_slide], f\"topK_mMV_{topK_mMV}\")\n",
    "    else:\n",
    "        save_dir = join(\"../../TEST_DATA_RESULTS\", experiment, \"search_result_images\", \"wsi_vertical\", query_slides_proxies[test_slide], f\"topK_mMV_{topK_mMV}\")\n",
    "    makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    query_path = join(TEST_WSI_DIR, experiment, f\"{test_slide}.{extension}\")\n",
    "    query_slide = openslide.open_slide(query_path)\n",
    "    query_thumbnail = query_slide.get_thumbnail((300, 300))\n",
    "    query_slide.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))  # adjust as necessary\n",
    "\n",
    "    plt.subplot(1, len(ret_final) + 2, 1)\n",
    "    plt.imshow(query_thumbnail)\n",
    "    plt.axis('off')  # to hide the x and y axis\n",
    "    plt.title(f\"Query\\n{query_slides_proxies[test_slide]}\")\n",
    "\n",
    "    # Plot black line\n",
    "    plt.subplot(1, len(ret_final) + 2, 2)\n",
    "    plt.plot([0, 0], [0, 1], color='black', transform=plt.gca().transAxes, linewidth=2.0)\n",
    "    plt.axis('off')\n",
    "\n",
    "    links = []\n",
    "    for i, result in enumerate(ret_final, 2):\n",
    "        file_name, sim, site = result\n",
    "        file_name = file_name + \".svs\"\n",
    "\n",
    "        site = sites_dict[metadata.loc[file_name, \"primary_site\"]]\n",
    "        diagnosis = diagnoses_dict[metadata.loc[file_name, \"project_name\"]]\n",
    "        \n",
    "        # path = join(\"/home/data/GDC_BBCLL\", metadata.loc[file_name, \"id\"], file_name)\n",
    "        path = join(WSI_DIR, site, diagnosis, file_name)\n",
    "        slide = openslide.open_slide(path)\n",
    "        thumbnail = slide.get_thumbnail((300, 300))\n",
    "        slide.close()\n",
    "\n",
    "        plt.subplot(1, len(ret_final) + 2, i)\n",
    "        plt.imshow(thumbnail)\n",
    "        plt.axis('off')  # to hide the x and y axis\n",
    "        plt.title(f'Result {i-2}: {site} - {diagnosis}\\n Hamming Dist: {sim :.1f}')  # add caption\n",
    "\n",
    "        # returning GDC link to slides\n",
    "        links.append(VIEW_URL + metadata.loc[file_name, \"id\"])\n",
    "\n",
    "        # saving to file\n",
    "        thumbnail.save(join(save_dir, f\"result_{i-1}.png\"))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig(join(save_dir, \"all.png\"), bbox_inches='tight')\n",
    "    fig.savefig(join(save_dir, \"all.eps\"), format='eps', bbox_inches='tight')\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564718e-f772-4697-a3d0-449fd9b46b4e",
   "metadata": {},
   "source": [
    "## Patch Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81da273-a0ca-4a9b-8c0a-c0a451e24734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_patch(query_slides, experiment, patch_label_file_path, database_index_path, index_meta_path, save_name, pre_step, succ_step, C, T, thrsh, codebook_semantic=\"../../checkpoints/codebook_semantic.pt\"):\n",
    "    \n",
    "    save_dir = join(TEST_DATA_DIR, experiment, \"_Results_Patch\")\n",
    "    makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    patch_label_file = pd.read_csv(patch_label_file_path)\n",
    "    db = HistoDatabase(database_index_path, index_meta_path, codebook_semantic, is_patch=True)\n",
    "    db.leave_test_slides(list(query_slides.keys()))\n",
    "    \n",
    "    results = {}\n",
    "    # for idx in range(len(patch_label_file)):\n",
    "    # [30594, 30595]\n",
    "    for idx in range(0, 12):\n",
    "        patch_name = patch_label_file.loc[idx, 'Patch Names']\n",
    "        label = patch_label_file.loc[idx, 'label']\n",
    "\n",
    "        latentfeat_path = join(TEST_DATA_DIR, experiment, \"DATA_PATCH\", f\"{experiment}_latent\", 'vqvae', patch_name + \".h5\")\n",
    "        densefeat_path = join(TEST_DATA_DIR, experiment, \"DATA_PATCH\", f\"{experiment}_latent\", 'densenet', patch_name + \".pkl\")\n",
    "\n",
    "        with h5py.File(latentfeat_path, 'r') as hf:\n",
    "            feat = hf['features'][:]\n",
    "        with open(densefeat_path, 'rb') as handle:\n",
    "            densefeat = pickle.load(handle)\n",
    "\n",
    "        tmp_res = db.query(feat[0], densefeat, pre_step, succ_step, C, T, thrsh)\n",
    "\n",
    "        tmp_clean = []\n",
    "        for r in tmp_res:\n",
    "            if r['patch_name'] == patch_name:\n",
    "                continue\n",
    "            else:\n",
    "                # tmp_clean.append((r['hamming_dist'], r['diagnosis'], r['patch_name']))\n",
    "                # tmp_clean.append(r)\n",
    "                tmp_clean.append(tuple(r.values()))\n",
    "\n",
    "        # top5 = sorted(tmp_clean, key=lambda x: x[0])\n",
    "        # top5 = sorted(tmp_clean, key=lambda x: x['hamming_dist'])\n",
    "        top5 = sorted(tmp_clean, key=lambda x: x[3])\n",
    "        results[patch_name] = {'results': None, 'label_query': None}\n",
    "        results[patch_name]['results'] = top5\n",
    "        results[patch_name]['label_query'] = label\n",
    "\n",
    "    with open(join(save_dir, f\"{save_name}.pkl\"), 'wb') as handle:\n",
    "        pickle.dump(results, handle)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b886543-6d35-48e4-8344-eaaa8307329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results_patch(test_slide, ret_final, metadata, experiment):\n",
    "    slide_name, patch_coord1, patch_coord2 = splitext(test_slide)[0].split(\"_\")\n",
    "    slide_name = query_slides_proxies[slide_name]\n",
    "    patch_name = f\"patch_{patch_coord1}_{patch_coord2}\"\n",
    "    save_dir = join(\"TEST_DATA_RESULTS\", experiment, \"search_result_images\", \"patch_retrieval\", slide_name, patch_name)\n",
    "    makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    test_slide = splitext(test_slide)[0]\n",
    "    query_slide = test_slide.split(\"_\")[0]\n",
    "    \n",
    "    query_path = join(TEST_WSI_DIR, experiment, query_slide + \".svs\")\n",
    "    slide = openslide.open_slide(query_path)\n",
    "    try:\n",
    "        objective_power = int(slide.properties['openslide.objective-power'])\n",
    "    except KeyError:\n",
    "        objective_power = 20\n",
    "    slide.close()\n",
    "        \n",
    "    query_file = join(TEST_DATA_DIR, experiment, \"PATCHES\", experiment, experiment, f\"{objective_power}x\", \"patches\", query_slide + \".h5\")\n",
    "\n",
    "    coord = [int(test_slide.split(\"_\")[1]), int(test_slide.split(\"_\")[2])]\n",
    "    \n",
    "    with h5py.File(query_file, 'r') as f:\n",
    "        dataset = f['coords']\n",
    "        patch_level = dataset.attrs[\"patch_level\"]\n",
    "        patch_size = dataset.attrs[\"patch_size\"]   \n",
    "    \n",
    "    slide = openslide.open_slide(query_path)\n",
    "    patch = slide.read_region((coord[0], coord[1]), patch_level, (patch_size, patch_size)).convert(\"RGB\")\n",
    "    slide.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10), dpi=1200)  # adjust as necessary\n",
    "\n",
    "    plt.subplot(1, len(ret_final) + 2, 1)\n",
    "    plt.imshow(patch)\n",
    "    plt.axis('off')  # to hide the x and y axis\n",
    "    plt.title(f\"Query Patch\\n{slide_name} - {patch_name}\")\n",
    "\n",
    "    # Plot black line\n",
    "    plt.subplot(1, len(ret_final) + 2, 2)\n",
    "    plt.plot([0, 0], [0, 1], color='black', transform=plt.gca().transAxes, linewidth=2.0)\n",
    "    plt.axis('off')\n",
    "\n",
    "    links = []\n",
    "    for i, result in enumerate(ret_final, 2):\n",
    "        slide_path, slide_name, sim, site, diagnosis, coords, patch_level, patch_size = result[6], result[5], result[3], result[7], result[8], result[9], result[10], result[11]\n",
    "        \n",
    "        slide = openslide.open_slide(slide_path)\n",
    "        patch = slide.read_region((coords[0], coords[1]), patch_level, (patch_size, patch_size)).convert(\"RGB\")\n",
    "        slide.close()\n",
    "\n",
    "        plt.subplot(1, len(ret_final) + 2, i)\n",
    "        plt.imshow(patch)\n",
    "        plt.axis('off')  # to hide the x and y axis\n",
    "        plt.title(f'Result {i-2}: {site} - {diagnosis}\\n Hamming Dist: {sim :.1f}')  # add caption\n",
    "\n",
    "        # returning GDC link to slides\n",
    "        links.append(VIEW_URL + metadata.loc[slide_name, \"id\"])\n",
    "\n",
    "        # saving to file\n",
    "        patch.save(join(save_dir, f\"result_{i-1}.png\"))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig(join(save_dir, \"all.png\"),  dpi='figure', bbox_inches='tight')\n",
    "    fig.savefig(join(save_dir, \"all.eps\"), format='eps', dpi='figure', bbox_inches='tight')\n",
    "    fig.savefig(join(save_dir, \"all.pdf\"), format='pdf', dpi='figure', bbox_inches='tight')\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79b2d7c-a5ab-4fcc-bace-3308fb37f54f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba478e11-7721-4bc1-ba34-962b0ffdf67b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diagnoses_dict = {\n",
    "    \"Brain Lower Grade Glioma\": \"LGG\",\n",
    "    \"Glioblastoma Multiforme\": \"GBM\",\n",
    "    \"Breast Invasive Carcinoma\": \"BRCA\",\n",
    "    \"Lung Adenocarcinoma\": \"LUAD\",\n",
    "    \"Lung Squamous Cell Carcinoma\": \"LUSC\",\n",
    "    \"Colon Adenocarcinoma\": \"COAD\",\n",
    "    \"Liver Hepatocellular Carcinoma\": \"LIHC\",\n",
    "    \"Cholangiocarcinoma\": \"CHOL\",\n",
    "}\n",
    "\n",
    "sites_dict = {\n",
    "    \"Brain\": \"brain\",\n",
    "    \"Breast\": \"breast\",\n",
    "    \"Bronchus and lung\": \"lung\",\n",
    "    \"Colon\": \"colon\",\n",
    "    \"Liver and intrahepatic bile ducts\": \"liver\",\n",
    "}\n",
    "\n",
    "sites_diagnoses_dict = {\n",
    "    \"brain\": [\"LGG\", \"GBM\"],\n",
    "    \"breast\": [\"BRCA\"],\n",
    "    \"lung\": [\"LUAD\", \"LUSC\"],\n",
    "    \"colon\": [\"COAD\"],\n",
    "    \"liver\": [\"LIHC\", \"CHOL\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a77bf7a-b478-4f48-8fa9-10e6243b1431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VIEW_URL = \"https://portal.gdc.cancer.gov/files/\"\n",
    "FILES_ENDPOINT = \"https://api.gdc.cancer.gov/files\"\n",
    "\n",
    "DATA_DIR = \"../../FEATURES/DATABASE/\"\n",
    "TEST_DATA_DIR = \"../../FEATURES/TEST_DATA/\"\n",
    "\n",
    "WSI_DIR = \"/raid/nejm_ai/DATABASE/\"\n",
    "TEST_WSI_DIR = \"/raid/nejm_ai/TEST_DATA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ccc8b-78d9-46b6-98d3-f088c6905a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"BRCA_HER2\"\n",
    "extension = \"svs\" # 'ndpi' for GBM_MICROSCOPE_UPENN, 'svs' everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38c6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../FEATURES/TEST_DATA/{experiment}/query_slides.yaml\", 'r') as f:\n",
    "    query_slides = yaml.safe_load(f)\n",
    "\n",
    "query_slides_proxies = dict()\n",
    "\n",
    "for key in query_slides.keys():\n",
    "    query_slides_proxies[splitext(key)[0]] = splitext(key)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b463d3-4b94-4cfe-b63b-ac80a128fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../../FEATURES/DATABASE/sampled_metadata.csv\")\n",
    "metadata = metadata.set_index('file_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0895e4-ae7b-4de3-b44c-da17f4b8bfe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_step = 375\n",
    "succ_step = 375\n",
    "C = 50\n",
    "T = 10\n",
    "thrsh = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21146a0e-e270-418d-bec0-31c4a8e4e16b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Saving the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb080d74-ba83-4575-a432-e02be4b9d984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only run once, then comment out.\n",
    "sites = [\"organ\", \"brain\", \"breast\", \"lung\", \"colon\", \"liver\"]\n",
    "for site in sites:\n",
    "    save_results(query_slides, extension, site, experiment, pre_step, succ_step, C, T, thrsh)\n",
    "\n",
    "# Patch results\n",
    "patch_label_file_path = f\"../../FEATURES/TEST_DATA/{experiment}/DATA_PATCH/summary.csv\"\n",
    "database_index_path = f\"../../FEATURES/TEST_DATA/{experiment}/DATABASES_PATCH/{experiment}/index_tree/veb.pkl\"\n",
    "index_meta_path = f\"../../FEATURES/TEST_DATA/{experiment}/DATABASES_PATCH/{experiment}/index_meta/meta.pkl\"\n",
    "save_name=\"results_READER_STUDY\"\n",
    "\n",
    "results = save_results_patch(query_slides, experiment, patch_label_file_path, database_index_path, index_meta_path, save_name, pre_step, succ_step, C, T, thrsh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81104724-b59f-45d8-9f62-4831dd68e0dc",
   "metadata": {},
   "source": [
    "# Organ Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee09d9e-e0e9-4e94-bc87-9cb8e76979f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topK_mMV = 10\n",
    "site = \"organ\"\n",
    "\n",
    "results = unpickle_object(f\"../../FEATURES/TEST_DATA/{experiment}/Results/{site}/results.pkl\")\n",
    "query_results = query(results, site, topK_mMV)\n",
    "\n",
    "for test_slide, ret_final in query_results:\n",
    "    if test_slide in query_slides_proxies:\n",
    "        show_results(test_slide, extension, ret_final, metadata, experiment, site_retrieval=True, topK_mMV=topK_mMV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551db42-7ce7-4285-92c9-758e90640b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topK_mMV = 5\n",
    "site = \"organ\"\n",
    "\n",
    "results = unpickle_object(f\"../../FEATURES/TEST_DATA/{experiment}/Results/{site}/results.pkl\")\n",
    "query_results = query(results, site, topK_mMV)\n",
    "\n",
    "for test_slide, ret_final in query_results:\n",
    "    if test_slide in query_slides_proxies:\n",
    "        show_results(test_slide, extension, ret_final, metadata, experiment, site_retrieval=True, topK_mMV=topK_mMV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55837f2d-915d-4cd8-b5fd-dc8762ab0cb6",
   "metadata": {},
   "source": [
    "# Sub-Type Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a6572-9ac6-4d31-8bc5-375fbf3c941b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topK_mMV = 5\n",
    "\n",
    "for slide, sites in query_slides.items():\n",
    "    for site in sites:\n",
    "        results = unpickle_object(f\"../../FEATURES/TEST_DATA/{experiment}/Results/{site}/results.pkl\")\n",
    "        query_results = query(results, site, topK_mMV)\n",
    "\n",
    "        for test_slide, ret_final in query_results:\n",
    "            if test_slide + \".svs\" == slide:\n",
    "                show_results(test_slide, extension, ret_final, metadata, experiment, site_retrieval=False, topK_mMV=topK_mMV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364759bd-456c-4fac-922b-dac6850010b0",
   "metadata": {},
   "source": [
    "# Patch Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92376db-45c0-420b-a381-3069f66279c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = unpickle_object(f\"./Results_Patch_READER_STUDY/results_READER_STUDY.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80b2b6-a181-4521-9b71-69e3ce9495c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patches = [\n",
    "    \"MSB-09151-01-11_23185_27168.json\",\n",
    "    \"MSB-09151-01-11_6801_10784.json\",\n",
    "    \"MSB-09151-01-11_21137_6688.json\",\n",
    "    \"MSB-09151-01-11_23185_6688.json\",\n",
    "]\n",
    "site = \"colon\"\n",
    "experiment = \"READER_STUDY\"\n",
    "\n",
    "topk_MV = 5\n",
    "\n",
    "for test_p in test_patches:\n",
    "    res_final = []\n",
    "    for result in results[test_p][\"results\"]:\n",
    "        if result[7] == site:\n",
    "            res_final.append(result)\n",
    "    res_final = res_final[:topk_MV]\n",
    "    links = show_results_patch(test_p, res_final, metadata, experiment)\n",
    "    print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4ece2-a2d5-4afc-b454-9897fd58b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patches = [\n",
    "    \"MSB-09977-01-22_19206_21062.json\",\n",
    "    \"MSB-09977-01-22_29929_53329.json\",\n",
    "    \"MSB-09977-01-22_74872_30600.json\",\n",
    "    \"MSB-09977-01-22_76920_32648.json\",\n",
    "]\n",
    "site = \"lung\"\n",
    "experiment = \"READER_STUDY\"\n",
    "\n",
    "topk_MV = 5\n",
    "\n",
    "for test_p in test_patches:\n",
    "    res_final = []\n",
    "    for result in results[test_p][\"results\"]:\n",
    "        if result[7] == site:\n",
    "            res_final.append(result)\n",
    "    res_final = res_final[:topk_MV]\n",
    "    links = show_results_patch(test_p, res_final, metadata, experiment)\n",
    "    print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092137c-ef99-405d-8078-6c3918718138",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patches = [\n",
    "    \"Her2Pos_Case_66_3072_13472.json\",\n",
    "    \"Her2Pos_Case_66_19456_1184.json\",\n",
    "    \"Her2Pos_Case_66_25328_12291.json\",\n",
    "    \"Her2Pos_Case_66_27376_9219.json\",\n",
    "]\n",
    "site = \"breast\"\n",
    "experiment = \"READER_STUDY\"\n",
    "\n",
    "topk_MV = 5\n",
    "\n",
    "for test_p in test_patches:\n",
    "    res_final = []\n",
    "    for result in results[test_p][\"results\"]:\n",
    "        if result[7] == site:\n",
    "            res_final.append(result)\n",
    "    res_final = res_final[:topk_MV]\n",
    "    links = show_results_patch(test_p, res_final, metadata, experiment)\n",
    "    print(links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
