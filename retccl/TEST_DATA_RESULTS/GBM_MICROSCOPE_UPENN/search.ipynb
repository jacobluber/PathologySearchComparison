{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2edf1-b162-4b75-b6da-626792b06db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import join, basename, splitext\n",
    "from statistics import mode, mean\n",
    "from collections import Counter\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import openslide\n",
    "from matplotlib import pyplot as plt\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from model import ccl_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378dbfe3-1c45-4841-a763-b44711aa264f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac7752-b4e8-4c82-9b94-11bb3d48b5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b)/(norm(a) * norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52603d-8fd4-49f8-a1e0-346910aa7399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_weights(site):\n",
    "    if site == \"organ\":\n",
    "        factor = 30\n",
    "        # Count the number of slide in each diagnosis (organ)\n",
    "        latent_all = join(PATCHES_DIR, \"*\", \"*\", \"patches\", \"*\")\n",
    "        type_of_organ = list(sites_diagnoses_dict.keys())\n",
    "        total_slide = {k: 0 for k in type_of_organ}\n",
    "        for latent_path in glob.glob(latent_all):\n",
    "            anatomic_site = latent_path.split(\"/\")[-4]\n",
    "            total_slide[anatomic_site] += 1\n",
    "    else:\n",
    "        factor = 10\n",
    "        # Count the number of slide in each site (organ)\n",
    "        latent_all = join(PATCHES_DIR, site, \"*\", \"patches\", \"*\")\n",
    "        type_of_diagnosis = sites_diagnoses_dict[site]\n",
    "        total_slide = {k: 0 for k in type_of_diagnosis}\n",
    "        for latent_path in glob.glob(latent_all):\n",
    "            diagnosis = latent_path.split(\"/\")[-3]\n",
    "            total_slide[diagnosis] += 1\n",
    "    \n",
    "    # Using the inverse count as a weight for each diagnosis\n",
    "    sum_inv = 0\n",
    "    for v in total_slide.values():\n",
    "        sum_inv += (1./v)\n",
    "\n",
    "    # Set a parameter k  to make the weight sum to k (k = 10, here)\n",
    "    norm_fact = factor / sum_inv\n",
    "    weight = {k: norm_fact * 1./v for k, v in total_slide.items()}\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e215c-1baa-422f-aca4-308a3c26b90e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wsi_query(mosaics, test_mosaics, metadata, site, weight, cosine_threshold, results_dir, temp_results_dir):\n",
    "    Bags = {}    # Dictionary to store each Bag for each query WSI\n",
    "    Entropies = {}    # Dictionary to store entropies for each patch in each Bag for each query WSI\n",
    "    Etas = {}    # Dictionary to store eta thresholds for each patch in each Bag for each query WSI\n",
    "    Results = {}    # Dictionary to store top-N similar WSIs to query WSI\n",
    "    for fname in test_mosaics.index.unique():\n",
    "        WSI = test_mosaics.loc[fname][\"features\"].tolist()\n",
    "        k = len(WSI)\n",
    "        Bag = {}\n",
    "        Entropy = {}\n",
    "        for patch_idx, patch_feature in enumerate(WSI):\n",
    "            # Retreiving similar patches (creating Bag)\n",
    "            if site == \"organ\":\n",
    "                bag = [(idx, cosine_sim(patch_feature, row[\"features\"])) for idx, row in mosaics.iterrows() if cosine_sim(patch_feature, row[\"features\"]) >= cosine_threshold]\n",
    "            else:\n",
    "                site_mosaics = mosaics.loc[list(metadata.loc[mosaics.loc[:, \"file_name\"], \"primary_site\"].apply(lambda x: sites_dict[x]) == site)].copy()\n",
    "                bag = [(idx, cosine_sim(patch_feature, row[\"features\"])) for idx, row in site_mosaics.iterrows() if cosine_sim(patch_feature, row[\"features\"]) >= cosine_threshold]\n",
    "            Bag[patch_idx] = sorted(bag, key=lambda x: x[1], reverse=True)\n",
    "            t = len(Bag[patch_idx])\n",
    "\n",
    "            # Calculating entropy for each query patch in the Bag\n",
    "            entropy = 0\n",
    "            if site == \"organ\":\n",
    "                u = set([sites_dict[metadata.loc[mosaics.loc[idx, \"file_name\"], \"primary_site\"]] for (idx, _) in Bag[patch_idx]])\n",
    "                for organ in u:\n",
    "                    num, denum = 0, 0\n",
    "                    for (idx, sim) in Bag[patch_idx]:\n",
    "                        bag_organ = sites_dict[metadata.loc[mosaics.loc[idx, \"file_name\"], \"primary_site\"]]\n",
    "                        num += ((organ==bag_organ) * 1) * ((sim + 1) / 2) * weight[bag_organ]\n",
    "                        denum += ((sim + 1) / 2) * weight[bag_organ]\n",
    "                    p = num / denum\n",
    "                    entropy -= p * np.log(p)\n",
    "            else:\n",
    "                u = set([diagnoses_dict[metadata.loc[site_mosaics.loc[idx, \"file_name\"], \"project_name\"]] for (idx, _) in Bag[patch_idx]])\n",
    "                for diagnosis in u:\n",
    "                    num, denum = 0, 0\n",
    "                    for (idx, sim) in Bag[patch_idx]:\n",
    "                        bag_diagnosis = diagnoses_dict[metadata.loc[site_mosaics.loc[idx, \"file_name\"], \"project_name\"]]\n",
    "                        num += ((diagnosis==bag_diagnosis) * 1) * ((sim + 1) / 2) * weight[bag_diagnosis]\n",
    "                        denum += ((sim + 1) / 2) * weight[bag_diagnosis]\n",
    "                    p = num / denum\n",
    "                    entropy -= p * np.log(p)\n",
    "            Entropy[patch_idx] = entropy\n",
    "            \n",
    "        # Sorting Bag members in terms of descending entropy\n",
    "        Bag = dict(sorted(Bag.items(), key=lambda x: Entropy[x[0]], reverse=True))\n",
    "\n",
    "        # Calculating eta threshold for each query patch in the Bag\n",
    "        eta_threshold = 0\n",
    "        for patch_idx in range(len(WSI)):\n",
    "            eta = np.mean([x[1] for x in Bag[patch_idx][:5]]) if len(Bag[patch_idx]) else 0\n",
    "            # eta = 0 if np.isnan(eta) else eta\n",
    "            eta_threshold += eta \n",
    "        eta_threshold = eta_threshold / k\n",
    "\n",
    "        # Removing query patches in the Bag with small eta (eta < eta_threshold) \n",
    "        ids = []\n",
    "        for idx, bag in Bag.items():\n",
    "            eta = np.mean([x[1] for x in bag[:5]]) if len(bag) else 0\n",
    "            # eta = 0 if np.isnan(eta) else eta\n",
    "            if eta < eta_threshold:\n",
    "                ids.append(idx)\n",
    "        for idx in ids:\n",
    "            del Bag[idx]\n",
    "\n",
    "        # Majority voting for retrieving the results\n",
    "        WSIRet = {}\n",
    "        for idx, bag in Bag.items():\n",
    "            if site == \"organ\":\n",
    "                matches = [sites_dict[metadata.loc[mosaics.loc[b[0], \"file_name\"], \"primary_site\"]] for b in bag[:5]]\n",
    "                slides = [mosaics.loc[b[0], \"slide_path\"] for b in bag[:5]]\n",
    "            else:\n",
    "                matches = [diagnoses_dict[metadata.loc[site_mosaics.loc[b[0], \"file_name\"], \"project_name\"]] for b in bag[:5]]\n",
    "                slides = [site_mosaics.loc[b[0], \"slide_path\"] for b in bag[:5]]\n",
    "            sims = [b[1] for b in bag[:5]]\n",
    "            # Using slide path as the key\n",
    "            slide_path = slides[matches.index(mode(matches))]\n",
    "            if slide_path not in WSIRet:\n",
    "                WSIRet[slide_path] = (slide_path, sims[matches.index(mode(matches))], mean(sims))\n",
    "        WSIRet = list(WSIRet.values())\n",
    "\n",
    "        with open(join(temp_results_dir, f\"{splitext(fname)[0]}_bag.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(Bag, f)\n",
    "        with open(join(temp_results_dir, f\"{splitext(fname)[0]}_entropy.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(Entropy, f)\n",
    "        with open(join(temp_results_dir, f\"{splitext(fname)[0]}_WSIRet.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(WSIRet, f)\n",
    "\n",
    "        Bags[fname] = Bag\n",
    "        Entropies[fname] = Entropy\n",
    "        Etas[fname] = eta\n",
    "        Results[fname] = WSIRet\n",
    "\n",
    "    with open(join(results_dir, f\"Bags.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(Bags, f)\n",
    "    with open(join(results_dir, f\"Entropies.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(Entropies, f)\n",
    "    with open(join(results_dir, f\"Etas.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(Etas, f)\n",
    "    with open(join(results_dir, f\"Results.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(Results, f)\n",
    "        \n",
    "    return Results, Bags, Entropies, Etas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187f6a8-783d-429f-9369-aba636c0031e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_results(test_slide, ret_final, experiment, site_retrieval):\n",
    "    if site_retrieval:\n",
    "        save_dir = join(\"../../TEST_DATA_RESULTS\", experiment, \"search_result_images\", \"wsi_site_retrieval\", splitext(test_slide)[0])\n",
    "    else:\n",
    "        save_dir = join(\"../../TEST_DATA_RESULTS\", experiment, \"search_result_images\", \"wsi_vertical\", splitext(test_slide)[0])\n",
    "    makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    query_path = join(TEST_SLIDES_DIR, test_slide)\n",
    "    query_slide = openslide.open_slide(query_path)\n",
    "    query_thumbnail = query_slide.get_thumbnail((300, 300))\n",
    "    query_slide.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))  # adjust as necessary\n",
    "\n",
    "    plt.subplot(1, len(ret_final) + 2, 1)\n",
    "    plt.imshow(query_thumbnail)\n",
    "    plt.axis('off')  # to hide the x and y axis\n",
    "    plt.title(f\"Query\\n{splitext(test_slide)[0]}\")\n",
    "\n",
    "    # Plot black line\n",
    "    plt.subplot(1, len(ret_final) + 2, 2)\n",
    "    plt.plot([0, 0], [0, 1], color='black', transform=plt.gca().transAxes, linewidth=2.0)\n",
    "    plt.axis('off')\n",
    "\n",
    "    links = []\n",
    "    for i, result in enumerate(ret_final, 2):\n",
    "        path, sim, mean_sim = result\n",
    "        file_name = basename(path)\n",
    "        site = sites_dict[metadata.loc[file_name, \"primary_site\"]]\n",
    "        diagnosis = diagnoses_dict[metadata.loc[file_name, \"project_name\"]]\n",
    "        slide = openslide.open_slide(path)\n",
    "        thumbnail = slide.get_thumbnail((300, 300))\n",
    "        slide.close()\n",
    "\n",
    "        plt.subplot(1, len(ret_final) + 2, i)\n",
    "        plt.imshow(thumbnail)\n",
    "        plt.axis('off')  # to hide the x and y axis\n",
    "        plt.title(f'Result {i-2}: {site} - {diagnosis}')  # add caption\n",
    "        plt.title(f'Result {i-1}: {site} - {diagnosis}\\nTop Mean Similarity: {mean_sim:.2f}')\n",
    "\n",
    "        # returning GDC link to slides\n",
    "        links.append(VIEW_URL + metadata.loc[file_name, \"id\"])\n",
    "\n",
    "        # saving to file\n",
    "        thumbnail.save(join(save_dir, f\"result_{i-1}.png\"))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig(join(save_dir, \"all.png\"), bbox_inches='tight')\n",
    "    fig.savefig(join(save_dir, \"all.eps\"), format='eps', bbox_inches='tight')\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4c458-f16d-42c2-bbbf-442f86d95ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features(slide, patch):\n",
    "    class roi_dataset(Dataset):\n",
    "        def __init__(self, slide, patch, transforms):\n",
    "            super().__init__()\n",
    "            self.patch = patch\n",
    "            self.slide = slide\n",
    "            self.transforms = transforms\n",
    "\n",
    "        def __len__(self):\n",
    "            return 1\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            path = join(TEST_PATCHES_DIR, splitext(self.slide)[0], self.patch)\n",
    "            patch_region = Image.open(path)            \n",
    "            patch_region = self.transforms(patch_region)\n",
    "            return patch_region\n",
    "        \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    trnsforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    \n",
    "    dataset = roi_dataset(slide, patch, trnsforms)\n",
    "    database_loader = DataLoader(dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "    ccl = ccl_model(checkpoint_path=\"../../checkpoints/best_ckpt.pth\").to(device)\n",
    "    ccl.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in database_loader:\n",
    "            batch = batch.to(device)\n",
    "            features = ccl(batch)\n",
    "\n",
    "    return features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ea9f3-2d40-4936-a684-f46939825b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_query(test_slide, patch, mosaics, metadata, site, cosine_threshold):\n",
    "    _, coord1, coord2 = splitext(patch)[0].split(\"_\")\n",
    "    patch_feature = get_features(test_slide, patch)\n",
    "    \n",
    "    site_mosaics = mosaics.loc[list(metadata.loc[mosaics.loc[:, \"file_name\"], \"primary_site\"].apply(lambda x: sites_dict[x]) == site)].copy()\n",
    "    \n",
    "    bag = [(idx, cosine_sim(patch_feature, row[\"features\"]), row[\"patch_level\"], row[\"patch_size\"], row[\"coord1\"], row[\"coord2\"]) for idx, row in site_mosaics.iterrows() if cosine_sim(patch_feature, row[\"features\"]) >= cosine_threshold]\n",
    "    Bag = sorted(bag, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    slides = [site_mosaics.loc[b[0], \"slide_path\"] for b in Bag]\n",
    "    meta = [b[1:] for b in Bag]\n",
    "    Results = [(slide, sim, level, patch_size, coord1, coord2) for slide, (sim, level, patch_size, coord1, coord2) in zip(slides, meta)]\n",
    "    return Results, Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80395c24-d0bf-4e09-954c-85dfc336a6a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_results_patch(test_slide, patch, results, experiment):\n",
    "    save_dir = join(\"../../TEST_DATA_RESULTS\", experiment, \"search_result_images\", \"patch_retrieval\", splitext(test_slide)[0], splitext(patch)[0])\n",
    "    makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    query_path = join(TEST_PATCHES_DIR, splitext(test_slide)[0], patch)\n",
    "    query_thumbnail = Image.open(query_path)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 10), dpi=1200)  # adjust as necessary\n",
    "\n",
    "    plt.subplot(1, len(results) + 2, 1)\n",
    "    plt.imshow(np.array(query_thumbnail))\n",
    "    plt.axis('off')  # to hide the x and y axis\n",
    "    plt.title(f\"Query\\n{test_slide}\")\n",
    "\n",
    "    # Plot black line\n",
    "    plt.subplot(1, len(results) + 2, 2)\n",
    "    plt.plot([0, 0], [0, 1], color='black', transform=plt.gca().transAxes, linewidth=2.0)\n",
    "    plt.axis('off')\n",
    "\n",
    "    links = []\n",
    "    for i, result in enumerate(results, 2):\n",
    "        slide_path, sim, level, patch_size, coord1, coord2 = result\n",
    "        file_name = basename(slide_path)\n",
    "        slide = openslide.OpenSlide(slide_path)\n",
    "        region = slide.read_region((coord1, coord2), level, (patch_size, patch_size)).convert(\"RGB\")\n",
    "        slide.close()\n",
    "\n",
    "        plt.subplot(1, len(results) + 2, i)\n",
    "        plt.imshow(np.array(region))\n",
    "        plt.axis('off')  # to hide the x and y axis\n",
    "        site = sites_dict[metadata.loc[file_name, \"primary_site\"]]\n",
    "        diagnosis = diagnoses_dict[metadata.loc[file_name, \"project_name\"]]\n",
    "        plt.title(f'Result {i-1}: {site} - {diagnosis}\\nCosine Similarity: {float(sim): .2f}')  # add caption\n",
    "\n",
    "        # returning GDC link to slides\n",
    "        links.append(VIEW_URL + metadata.loc[file_name, \"id\"])\n",
    "\n",
    "        # saving to file\n",
    "        region.save(join(save_dir, f\"result_{i-1}.png\"))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig(join(save_dir, \"all.png\"),  dpi='figure', bbox_inches='tight')\n",
    "    fig.savefig(join(save_dir, \"all.eps\"), format='eps', dpi='figure', bbox_inches='tight')\n",
    "    fig.savefig(join(save_dir, \"all.pdf\"), format='pdf', dpi='figure', bbox_inches='tight')\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5805884a-668b-4021-b2ec-bec2aebce8be",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f929b-5d01-4509-9495-c48ca3bade11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diagnoses_dict = {\n",
    "    \"Brain Lower Grade Glioma\": \"LGG\",\n",
    "    \"Glioblastoma Multiforme\": \"GBM\",\n",
    "    \"Breast Invasive Carcinoma\": \"BRCA\",\n",
    "    \"Lung Adenocarcinoma\": \"LUAD\",\n",
    "    \"Lung Squamous Cell Carcinoma\": \"LUSC\",\n",
    "    \"Colon Adenocarcinoma\": \"COAD\",\n",
    "    \"Liver Hepatocellular Carcinoma\": \"LIHC\",\n",
    "    \"Cholangiocarcinoma\": \"CHOL\",\n",
    "}\n",
    "\n",
    "sites_dict = {\n",
    "    \"Brain\": \"brain\",\n",
    "    \"Breast\": \"breast\",\n",
    "    \"Bronchus and lung\": \"lung\",\n",
    "    \"Colon\": \"colon\",\n",
    "    \"Liver and intrahepatic bile ducts\": \"liver\",\n",
    "}\n",
    "\n",
    "sites_diagnoses_dict = {\n",
    "    \"brain\": [\"LGG\", \"GBM\"],\n",
    "    \"breast\": [\"BRCA\"],\n",
    "    \"lung\": [\"LUAD\", \"LUSC\"],\n",
    "    \"colon\": [\"COAD\"],\n",
    "    \"liver\": [\"LIHC\", \"CHOL\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255a46c-1b12-4d82-a694-672261f14d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIEW_URL = \"https://portal.gdc.cancer.gov/files/\"\n",
    "PATCHES_DIR = \"../../FEATURES/DATABASE\"\n",
    "TEST_SLIDES_DIR = \"/raid/nejm_ai/TEST_DATA/GBM_MICROSCOPE_UPENN\"\n",
    "TEST_PATCHES_DIR = \"../../FEATURES/TEST_DATA/GBM_MICROSCOPE_UPENN/visualized_patches\"\n",
    "RESULTS_DIR = \"../../FEATURES/TEST_DATA/GBM_MICROSCOPE_UPENN/results\"\n",
    "query_slides_path = \"../../FEATURES/TEST_DATA/GBM_MICROSCOPE_UPENN/query_slides.yaml\"\n",
    "\n",
    "experiment = \"GBM_MICROSCOPE_UPENN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a606d-aa52-40d9-930f-d931e75c4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(query_slides_path, 'r') as f:\n",
    "    query_slides = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1faf35-64b1-4f45-8928-bcad21aa0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"../../FEATURES/DATABASE/sampled_metadata.csv\"\n",
    "mosaics_path = \"../../FEATURES/DATABASE/mosaics.h5\"\n",
    "test_mosaics_path = \"../../FEATURES/TEST_DATA/GBM_MICROSCOPE_UPENN/mosaics.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb565f-b144-4864-b285-2cfd6d87729b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata = metadata.set_index('file_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650092d-69c6-403f-a596-9c7dda56f224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mosaics = pd.read_hdf(mosaics_path, 'df')\n",
    "test_mosaics = pd.read_json(test_mosaics_path)\n",
    "test_mosaics[\"features\"] = test_mosaics[\"features\"].apply(lambda lst: torch.tensor(lst))\n",
    "test_mosaics[\"file_name\"] = test_mosaics.apply(lambda row: basename(row[\"slide_path\"]), axis=1)\n",
    "test_mosaics = test_mosaics.set_index(['file_name'], inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1712d-1d87-4831-bdcf-2ee06fe7892c",
   "metadata": {},
   "source": [
    "# WSI Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d59fdf7-8f6c-4bbd-949d-92642fdb7636",
   "metadata": {},
   "source": [
    "## WSI Organ Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9afd00-01bc-4b01-9df7-906c8be8fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = join(RESULTS_DIR, \"organ\")\n",
    "temp_results_dir = join(results_dir, \"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0161ba8-095a-4455-a62e-9633106234ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(join(results_dir, \"Results.pkl\"), \"rb\") as file:\n",
    "    Results = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526fcffb-0821-4ffc-8f96-60b16c07ec38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 5    # N for top-N results\n",
    "for test_slide, ret_final in Results.items():\n",
    "    links = show_results(test_slide, ret_final[:N], experiment=experiment, site_retrieval=True)\n",
    "    print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc6c8a0-7e04-478f-ab2e-a0bd3a8a7cba",
   "metadata": {},
   "source": [
    "## WSI Sub-Type Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314018b-da7c-4635-a89c-0701510cc5a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for query, site in query_slides.items():\n",
    "    results_dir = join(RESULTS_DIR, f\"{splitext(query)[0]}_{site}\")\n",
    "    with open(join(results_dir, \"Results.pkl\"), \"rb\") as file:\n",
    "        Results = pickle.load(file)\n",
    "\n",
    "    N = 5    # N for top-N results\n",
    "    for test_slide, ret_final in Results.items():\n",
    "        links = show_results(test_slide, ret_final[:N], experiment=experiment, site_retrieval=False)\n",
    "        print(links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
